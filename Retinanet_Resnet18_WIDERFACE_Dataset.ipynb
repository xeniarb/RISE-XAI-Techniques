{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RetinaNet with ResNet18 Backbone – WIDER FACE Dataset (v1)\n",
        "\n",
        "This notebook demonstrates downloading and preparing the WIDER FACE dataset,\n",
        "building a lightweight RetinaFace architecture using a ResNet18 backbone,\n",
        "and training the model for facial detection tasks.\n",
        "\n",
        "References:\n",
        "- WIDER FACE Dataset: http://shuoyang1213.me/WIDERFACE/\n",
        "- Dataset scripts: https://huggingface.co/datasets/CUHK-CSE/wider_face\n",
        "- RetinaFace PyTorch Implementation: https://github.com/zisianw/FaceBoxes.PyTorch\n"
      ],
      "metadata": {
        "id": "tICUAGkBKqTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# =========================================================\n",
        "# 1. Import libraries\n",
        "# =========================================================\n",
        "```"
      ],
      "metadata": {
        "id": "LFeeBeUKNgS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import random"
      ],
      "metadata": {
        "id": "eVdSVaRocT-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# =========================================================\n",
        "# 2. Download and extract WIDER FACE dataset\n",
        "# =========================================================\n",
        "```"
      ],
      "metadata": {
        "id": "ewdZGcGvNwAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URLs for WIDER FACE dataset files\n",
        "_URLS = {\n",
        "   \"train\": \"https://huggingface.co/datasets/wider_face/resolve/main/data/WIDER_train.zip\",\n",
        "   \"validation\": \"https://huggingface.co/datasets/wider_face/resolve/main/data/WIDER_val.zip\",\n",
        "   \"test\": \"https://huggingface.co/datasets/wider_face/resolve/main/data/WIDER_test.zip\",\n",
        "   \"annot\": \"https://huggingface.co/datasets/wider_face/resolve/main/data/wider_face_split.zip\",\n",
        "}\n",
        "\n",
        "# Directory to save the downloaded files\n",
        "DATA_DIR = \"wider_face_data\"\n",
        "\n",
        "def download_and_extract(url, dest_folder):\n",
        "   # Get the filename from the URL\n",
        "   filename = os.path.join(dest_folder, url.split(\"/\")[-1])\n",
        "\n",
        "   # Download the file\n",
        "   print(f\"Downloading {filename}...\")\n",
        "   response = requests.get(url, stream=True)\n",
        "   response.raise_for_status()  # Check for errors\n",
        "\n",
        "   # Save the file\n",
        "   with open(filename, \"wb\") as file:\n",
        "       for chunk in response.iter_content(chunk_size=8192):\n",
        "           file.write(chunk)\n",
        "   print(f\"Downloaded {filename}.\")\n",
        "\n",
        "   # Extract the file\n",
        "   print(f\"Extracting {filename}...\")\n",
        "   with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
        "       zip_ref.extractall(dest_folder)\n",
        "   print(f\"Extracted {filename}.\")\n",
        "\n",
        "   # Optionally, delete the zip file after extraction\n",
        "   os.remove(filename)\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Download and extract each file\n",
        "for name, url in _URLS.items():\n",
        "   download_and_extract(url, DATA_DIR)\n",
        "\n",
        "print(\"All files downloaded and extracted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcxCNNDuJGEr",
        "outputId": "868dbd93-06c4-44a6-f67e-f070fbac838e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wider_face_data/WIDER_train.zip...\n",
            "Downloaded wider_face_data/WIDER_train.zip.\n",
            "Extracting wider_face_data/WIDER_train.zip...\n",
            "Extracted wider_face_data/WIDER_train.zip.\n",
            "Downloading wider_face_data/WIDER_val.zip...\n",
            "Downloaded wider_face_data/WIDER_val.zip.\n",
            "Extracting wider_face_data/WIDER_val.zip...\n",
            "Extracted wider_face_data/WIDER_val.zip.\n",
            "Downloading wider_face_data/WIDER_test.zip...\n",
            "Downloaded wider_face_data/WIDER_test.zip.\n",
            "Extracting wider_face_data/WIDER_test.zip...\n",
            "Extracted wider_face_data/WIDER_test.zip.\n",
            "Downloading wider_face_data/wider_face_split.zip...\n",
            "Downloaded wider_face_data/wider_face_split.zip.\n",
            "Extracting wider_face_data/wider_face_split.zip...\n",
            "Extracted wider_face_data/wider_face_split.zip.\n",
            "All files downloaded and extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_classification_loss(predictions, targets):\n",
        "    total_loss = 0\n",
        "    num_scales = len(predictions)\n",
        "    batch_size = predictions[0].size(0)\n",
        "\n",
        "    for scale in range(num_scales):\n",
        "        scale_preds = predictions[scale]  # Shape: [1, 2, H, W]\n",
        "\n",
        "        # Reshape predictions to [1, 2*H*W]\n",
        "        scale_preds = scale_preds.view(batch_size, -1)\n",
        "\n",
        "        # Create target tensor of the same shape as scale_preds\n",
        "        scale_targets = torch.zeros_like(scale_preds)\n",
        "\n",
        "        # Set positive samples (you need to implement this based on your data)\n",
        "        # For example, if you know which locations correspond to faces:\n",
        "        # scale_targets[:, face_locations] = 1\n",
        "\n",
        "        # Compute binary cross-entropy loss\n",
        "        loss = F.binary_cross_entropy_with_logits(scale_preds, scale_targets)\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "    return total_loss / num_scales\n",
        "\n",
        "def compute_bbox_loss(predictions, targets):\n",
        "    total_loss = 0\n",
        "    num_scales = len(predictions)\n",
        "    batch_size = predictions[0].size(0)\n",
        "\n",
        "    for scale in range(num_scales):\n",
        "        scale_preds = predictions[scale]  # Shape: [1, 8, H, W]\n",
        "\n",
        "        # Reshape predictions to [1, H*W, 4]\n",
        "        scale_preds = scale_preds.view(batch_size, -1, 4)\n",
        "\n",
        "        # Create target tensor of the same shape as scale_preds\n",
        "        scale_targets = torch.zeros_like(scale_preds)\n",
        "\n",
        "        # Set correct bounding boxes (you need to implement this based on your data)\n",
        "        # For example:\n",
        "        # scale_targets[:, face_locations, :] = correct_bboxes\n",
        "\n",
        "        # Compute smooth L1 loss\n",
        "        loss = F.smooth_l1_loss(scale_preds, scale_targets)\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "    return total_loss / num_scales\n",
        "\n",
        "def compute_landmark_loss(predictions, targets):\n",
        "    total_loss = 0\n",
        "    num_scales = len(predictions)\n",
        "    batch_size = predictions[0].size(0)\n",
        "\n",
        "    for scale in range(num_scales):\n",
        "        scale_preds = predictions[scale]  # Shape: [1, 20, H, W]\n",
        "\n",
        "        # Reshape predictions to [1, H*W, 10]\n",
        "        scale_preds = scale_preds.view(batch_size, -1, 10)\n",
        "\n",
        "        # Create target tensor of the same shape as scale_preds\n",
        "        scale_targets = torch.zeros_like(scale_preds)\n",
        "\n",
        "        # Set correct landmarks (you need to implement this based on your data)\n",
        "        # For example:\n",
        "        # scale_targets[:, face_locations, :] = correct_landmarks\n",
        "\n",
        "        # Compute smooth L1 loss\n",
        "        loss = F.smooth_l1_loss(scale_preds, scale_targets)\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "    return total_loss / num_scales\n",
        "\n",
        "def compute_total_loss(classifications, bbox_regressions, ldm_regressions, targets):\n",
        "    cls_loss = compute_classification_loss(classifications, targets)\n",
        "    bbox_loss = compute_bbox_loss(bbox_regressions, targets)\n",
        "    ldm_loss = compute_landmark_loss(ldm_regressions, targets)\n",
        "\n",
        "    lambda1 = 0.25\n",
        "    lambda2 = 0.1\n",
        "\n",
        "    total_loss = cls_loss + lambda1 * bbox_loss + lambda2 * ldm_loss\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "RvhJ_KDyk9AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# =========================================================\n",
        "# 3. Define custom dataset class\n",
        "# =========================================================\n",
        "```"
      ],
      "metadata": {
        "id": "z_z6HpvYOABW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WiderFaceDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, transform=None, subset_fraction=1.0):\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_dir = os.path.join(data_dir, f\"WIDER_{split}/images\")\n",
        "        if split != \"test\":\n",
        "            annot_file = os.path.join(data_dir, \"wider_face_split\", f\"wider_face_{split}_bbx_gt.txt\")\n",
        "            self.image_paths, self.annotations = self._load_annotations(annot_file)\n",
        "        else:\n",
        "            annot_file = os.path.join(data_dir, \"wider_face_split\", \"wider_face_test_filelist.txt\")\n",
        "            self.image_paths = self._load_test_images(annot_file)\n",
        "            self.annotations = None\n",
        "\n",
        "        # Sample a subset of data\n",
        "        if subset_fraction < 1.0:\n",
        "            subset_size = int(len(self.image_paths) * subset_fraction)\n",
        "            sampled_indices = random.sample(range(len(self.image_paths)), subset_size)\n",
        "            self.image_paths = [self.image_paths[i] for i in sampled_indices]\n",
        "            if self.annotations is not None:\n",
        "                self.annotations = [self.annotations[i] for i in sampled_indices]\n",
        "\n",
        "    def _load_annotations(self, annot_file):\n",
        "        \"\"\"Load image paths and bounding box annotations.\"\"\"\n",
        "        image_paths = []\n",
        "        annotations = []\n",
        "\n",
        "        with open(annot_file, \"r\") as f:\n",
        "            while True:\n",
        "                line = f.readline().strip()\n",
        "                if not line:  # End of file\n",
        "                    break\n",
        "                if \".jpg\" in line:  # Image filename\n",
        "                    image_paths.append(os.path.join(self.image_dir, line))\n",
        "                    num_boxes = int(f.readline().strip())\n",
        "                    boxes = []\n",
        "                    for _ in range(num_boxes):\n",
        "                        box_data = list(map(int, f.readline().strip().split()))\n",
        "                        boxes.append(box_data[:4])  # Extract xmin, ymin, width, height\n",
        "                    annotations.append(boxes)\n",
        "\n",
        "        return image_paths, annotations\n",
        "\n",
        "    def _load_test_images(self, annot_file):\n",
        "        \"\"\"Load test image paths.\"\"\"\n",
        "        with open(annot_file, \"r\") as f:\n",
        "            return [os.path.join(self.image_dir, line.strip()) for line in f if \".jpg\" in line]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Get original image size\n",
        "        orig_w, orig_h = image.size\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Adjust bounding boxes\n",
        "        if self.annotations is not None:\n",
        "            boxes = self.annotations[idx]\n",
        "            # Convert to relative coordinates and adjust for new size\n",
        "            adjusted_boxes = []\n",
        "            for box in boxes:\n",
        "                x, y, w, h = box\n",
        "                x_rel, y_rel = x / orig_w, y / orig_h\n",
        "                w_rel, h_rel = w / orig_w, h / orig_h\n",
        "                adjusted_boxes.append([x_rel * 640, y_rel * 640, w_rel * 640, h_rel * 640])\n",
        "\n",
        "            return {\"image\": image, \"boxes\": torch.tensor(adjusted_boxes, dtype=torch.float32)}\n",
        "\n",
        "        return {\"image\": image}"
      ],
      "metadata": {
        "id": "zZMFLnzFJKSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# =========================================================\n",
        "# 4. Data transformation and loading\n",
        "# =========================================================\n",
        "```"
      ],
      "metadata": {
        "id": "J3xa096rOJl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "xUE-uQrkKcb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to datasets (ensure you have downloaded and extracted them)\n",
        "DATA_DIR = \"wider_face_data\""
      ],
      "metadata": {
        "id": "HcEstT0PKa-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets with 10% of the original data\n",
        "train_dataset = WiderFaceDataset(data_dir=DATA_DIR, split=\"train\", transform=transform, subset_fraction=0.1)\n",
        "val_dataset = WiderFaceDataset(data_dir=DATA_DIR, split=\"val\", transform=transform, subset_fraction=0.1)"
      ],
      "metadata": {
        "id": "rYtOaTUgJWt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: print dataset size or access a sample\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "sample = train_dataset[0]\n",
        "print(f\"Sample image shape: {sample['image'].shape}\")\n",
        "print(f\"Sample bounding boxes: {sample['boxes']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxJKGVC9Ke_K",
        "outputId": "d1cb4176-1921-4b80-daf7-b4b4ee31fe30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1288\n",
            "Validation dataset size: 322\n",
            "Sample image shape: torch.Size([3, 640, 640])\n",
            "Sample bounding boxes: tensor([[180.6250, 127.5016, 278.7500, 207.6574]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# =========================================================\n",
        "# 5. Model definition\n",
        "# =========================================================\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZqkUOExwOQQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1   #bottleneck = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes * self.expansion, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes * self.expansion)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet18_Lightweight(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet18_Lightweight, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out1 = self.layer1(out)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        out4 = self.layer4(out3)\n",
        "        return out1, out2, out3, out4\n",
        "\n",
        "# Feature Pyramid Network (FPN)\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FPN, self).__init__()\n",
        "        self.toplayer = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.latlayer1 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer2 = nn.Conv2d(128, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer3 = nn.Conv2d(64, 256, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def _upsample_add(self, x, y):\n",
        "        _, _, H, W = y.size()\n",
        "        return F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False) + y\n",
        "\n",
        "    def forward(self, c1, c2, c3, c4):\n",
        "        p4 = self.toplayer(c4)\n",
        "        p3 = self._upsample_add(p4, self.latlayer1(c3))\n",
        "        p2 = self._upsample_add(p3, self.latlayer2(c2))\n",
        "        p1 = self._upsample_add(p2, self.latlayer3(c1))\n",
        "\n",
        "        p3 = self.smooth1(p3)\n",
        "        p2 = self.smooth2(p2)\n",
        "        p1 = self.smooth3(p1)\n",
        "\n",
        "        return p1, p2, p3, p4\n",
        "\n",
        "# Context Module\n",
        "class SSH(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SSH, self).__init__()\n",
        "        self.conv3X3 = nn.Conv2d(in_channels, out_channels//2, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5X5_1 = nn.Conv2d(in_channels, out_channels//4, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5X5_2 = nn.Conv2d(out_channels//4, out_channels//4, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv7X7_2 = nn.Conv2d(out_channels//4, out_channels//4, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv7x7_3 = nn.Conv2d(out_channels//4, out_channels//4, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv3X3 = self.conv3X3(x)\n",
        "        conv5X5_1 = self.conv5X5_1(x)\n",
        "        conv5X5 = self.conv5X5_2(conv5X5_1)\n",
        "        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n",
        "        conv7X7 = self.conv7x7_3(conv7X7_2)\n",
        "        return torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n",
        "\n",
        "# Lightweight RetinaFace Model\n",
        "class LightweightRetinaFace(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LightweightRetinaFace, self).__init__()\n",
        "        self.backbone = ResNet18_Lightweight()\n",
        "        self.fpn = FPN()\n",
        "        self.ssh1 = SSH(256, 256)\n",
        "        self.ssh2 = SSH(256, 256)\n",
        "        self.ssh3 = SSH(256, 256)\n",
        "        self.ssh4 = SSH(256, 256)\n",
        "\n",
        "        self.ClassHead = self._make_class_head()\n",
        "        self.BboxHead = self._make_bbox_head()\n",
        "        self.LandmarkHead = self._make_landmark_head()\n",
        "\n",
        "    def _make_class_head(self, fpn_num=4, inchannels=256, anchor_num=2):\n",
        "        classhead = nn.ModuleList()\n",
        "        for i in range(fpn_num):\n",
        "            classhead.append(nn.Conv2d(inchannels, anchor_num, kernel_size=1, stride=1, padding=0))\n",
        "        return classhead\n",
        "\n",
        "    def _make_bbox_head(self, fpn_num=4, inchannels=256, anchor_num=2):\n",
        "        bboxhead = nn.ModuleList()\n",
        "        for i in range(fpn_num):\n",
        "            bboxhead.append(nn.Conv2d(inchannels, anchor_num * 4, kernel_size=1, stride=1, padding=0))\n",
        "        return bboxhead\n",
        "\n",
        "    def _make_landmark_head(self, fpn_num=4, inchannels=256, anchor_num=2):\n",
        "        landmarkhead = nn.ModuleList()\n",
        "        for i in range(fpn_num):\n",
        "            landmarkhead.append(nn.Conv2d(inchannels, anchor_num * 10, kernel_size=1, stride=1, padding=0))\n",
        "        return landmarkhead\n",
        "\n",
        "    def forward(self, x):\n",
        "        c1, c2, c3, c4 = self.backbone(x)\n",
        "        p1, p2, p3, p4 = self.fpn(c1, c2, c3, c4)\n",
        "\n",
        "        feature1 = self.ssh1(p1)\n",
        "        feature2 = self.ssh2(p2)\n",
        "        feature3 = self.ssh3(p3)\n",
        "        feature4 = self.ssh4(p4)\n",
        "        features = [feature1, feature2, feature3, feature4]\n",
        "\n",
        "        bbox_regressions = []\n",
        "        classifications = []\n",
        "        ldm_regressions = []\n",
        "\n",
        "        for i, feature in enumerate(features):\n",
        "            bbox_regressions.append(self.BboxHead[i](feature))\n",
        "            classifications.append(self.ClassHead[i](feature))\n",
        "            ldm_regressions.append(self.LandmarkHead[i](feature))\n",
        "\n",
        "\n",
        "        return bbox_regressions, classifications, ldm_regressions"
      ],
      "metadata": {
        "id": "pDlM3NU7K2OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = LightweightRetinaFace().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define loss functions (you may need to implement custom loss functions for RetinaFace)\n",
        "classification_loss = nn.BCEWithLogitsLoss()\n",
        "bbox_loss = nn.SmoothL1Loss()\n",
        "landmark_loss = nn.SmoothL1Loss()\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "E0zaqqqbLAJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []\n",
        "    for item in batch:\n",
        "        images.append(item['image'])\n",
        "        targets.append({\n",
        "            'boxes': item['boxes'].clone().detach(),\n",
        "            'labels': torch.ones(len(item['boxes']), dtype=torch.float32)  # Assuming all are faces\n",
        "        })\n",
        "    return {'images': torch.stack(images), 'targets': targets}\n"
      ],
      "metadata": {
        "id": "fRzlVDQEfvWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# =========================================================\n",
        "# 6. Training and validation\n",
        "# =========================================================\n",
        "```"
      ],
      "metadata": {
        "id": "_FnhnJ0LOcu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUqUBAeUNWNO",
        "outputId": "ac60b7d1-4b86-427f-e1a9-1c7cb6a85273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SummaryWriter for TensorBoard\n",
        "writer = SummaryWriter('runs/retinaface_experiment')"
      ],
      "metadata": {
        "id": "wNN--4jIdoSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} Training\")\n",
        "    for batch in progress_bar:\n",
        "        images = batch['images'].to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in batch['targets']]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            bbox_regressions, classifications, ldm_regressions = model(images)\n",
        "            loss = compute_total_loss(classifications, bbox_regressions, ldm_regressions, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "lCnE3rjZd55Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, device, epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} Validation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            images = batch['images'].to(device)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in batch['targets']]\n",
        "\n",
        "            bbox_regressions, classifications, ldm_regressions = model(images)\n",
        "            loss = compute_total_loss(classifications, bbox_regressions, ldm_regressions, targets)\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    writer.add_scalar('Loss/val', avg_loss, epoch)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "Icsiw_Sbd_UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, device, epoch)\n",
        "    val_loss = validate(model, val_loader, device, epoch)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "    print(\"-----------------------\")\n",
        "\n",
        "    # Save the model checkpoint\n",
        "    torch.save(model.state_dict(), f'retinaface_epoch_{epoch+1}.pth')\n",
        "\n",
        "    # Log learning rate\n",
        "    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "W7xAjyW_eX3Y",
        "outputId": "c62fa38a-8ded-43a9-8c32-fb9948015308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Training: 100%|██████████| 322/322 [10:17<00:00,  1.92s/it, loss=0.000135]\n",
            "Epoch 1 Validation: 100%|██████████| 81/81 [02:06<00:00,  1.56s/it, loss=0.000394]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train Loss: 0.0409\n",
            "Validation Loss: 0.0052\n",
            "-----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Training: 100%|██████████| 322/322 [10:11<00:00,  1.90s/it, loss=5.59e-5]\n",
            "Epoch 2 Validation: 100%|██████████| 81/81 [02:05<00:00,  1.55s/it, loss=4.8e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50\n",
            "Train Loss: 0.0001\n",
            "Validation Loss: 0.0016\n",
            "-----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Training: 100%|██████████| 322/322 [10:10<00:00,  1.90s/it, loss=4.51e-5]\n",
            "Epoch 3 Validation: 100%|██████████| 81/81 [02:05<00:00,  1.55s/it, loss=8.79e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50\n",
            "Train Loss: 0.0001\n",
            "Validation Loss: 0.0016\n",
            "-----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Training:  60%|██████    | 194/322 [06:08<04:03,  1.90s/it, loss=4.88e-5]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-82a58e24d997>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d9c58186881f>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_regressions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mldm_regressions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# =========================================================\n",
        "# 7. Logging and Plotting\n",
        "# =========================================================\n",
        "```"
      ],
      "metadata": {
        "id": "XqQKS0jaOiyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss_plot.png')\n",
        "plt.show()\n",
        "\n",
        "# Close the TensorBoard writer\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "psZy0d6Seeau"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}